Sorting
	Sorting is the act of putting things in defined order
	Lexicographical order
	Numbers can be sorted in order
	Why?
		We can add info to our data
		We can now make assertions of before after and etc
		We can search merge and dupe detect more efficiently
	Bogosort
		Enum all possiblities of orderings
		Pick the one that is in order
		This sucks
	Second idea
		Find smallest
		Swap with first object
		Cut array size dwn by one
		Do again
		And again
		And again
	
	Fourth idea
		First two elemetns
		If first greater than second swap
		Move on to second and third
		Continue on until last pair
		What do we know?
			Last elemetn of the array is largest
			ignore it
			Repeat this proceducre on the first n-1
		But wait what happened if we make no swap
			it's sorted
		in the best case we can only examine n-1 pairs
		better bubble sort
		ONe pass order the array tells us if the array is sorted
		We can still do better/
			Prove with math that we can sort in time proportional to n log n
		Thought experiment
			Array with disjoint pairs
			we have n/2 pairs but look at both so call it n
			If we put the elements of every pair in order that is a run of length two
			Nowe we take a pair of runs and merge into runs of length 4
			How many times can we double 1 before we exceed n log n
			Our sort finished in time proportional to n log n
			This is merge sort
	Shell sort
		Organizes data on a course scale(large gap) first 
		Then goes to the small scale
	Quisk sort
		Cut array in half and stuff
		I need to firgure this out
		Computational complexity
		Cut array in half each time
		nlogn
		There exists for any choice of pivot element that is an adversarial array that makes complexity n^2
	Heaps
		Lot's of types of heaps
		A max heap
			a single node is a heap
			It is a heap if the parent is a heep and the trees rooted at both childs are heaps
			A parent's value (key) is greater than that of either child
			There is no order among the children
			Take the biggest child to the top and a nother child up
	Using comparisons the answer is no
	If we use assumptions about the encoding we can do radix sort
	Summary
		Sorting is fundamental
		Analysis of algorithm complexity is imprtant
		Better algorithm
		you will need to sort a lot in ur career
WHAT IS A POINTER
	Var that holds a mem address
	Point to the location of an object in memoery
	Not all ptrs contain an addresss
	PTRS that don't coontain an address are set to null ptr
	Value of the null pointer is 0
	Null is a macro for either ((void*)0) 
		0
		0L
		depends on the compiler
	Mem addresses
		Momory is in registers that can be accessed by a specifc number
		Pointers are pointing at the address they are assigned
		Can assign a pointer the address of a var using the address of operator
		Many pointers can do same address, called aliases.
	doing &something gives address of ptr
		int foo = 0
		print %p &foo is address of foo
De refferencing a pointer
	The object a ptr points to can be accessed through deference
	A pointer can be derefed using the deref operator
	Useful for manipulating things by call be reference
	& gets the addy of the thing
	How to fake call by reference'
You can say *bar = &foo;
	addy ofbar.
Passing by reference.
	Allows returning multiple values
	Passing large amounts of data quickly
Pointer arthmetic
	Pointers are just numbers
	++ increments to next addy (4bits) assuming 32 bit ints
	-- decrements to previous addy
	+ add a numeric value to a pointer (no ptr and ptr addition)
	CANT ADD PTR TO PTR
	- if a ptr is subtracted from another pointer, the distance between addy is calculated
	CAn be comared with relational ops.
	Cant sum divide or mult two pointers
	str are arrays
Array and pointer arith
	arr[i] == *(a+i)
	If start at 1000 then arr[i] is 100(i*4)
		an int is for bytes
	if arr was of uint 64_t then arr[i] would be *4 not *8
DOes is make sense?
	Adding int to ptr makes sense
	Subtracting two pointers makes sense
	Adding two pointers is dumb asf
	Multiply or dividing is also dumb asf
*(fib_ptr)++ derefe then increment
fib_ptr++; increment the pointer
Array subs can be don with pointerse
	pointer arithmetic in general is faster but harder to understand
	int arr[10]
		arr[i] is eq to *(arr+1), 0<= i <10
	Arrays can always be done with ptrs
	Declaring an array in a function allocates it on the stack
	A global array is in the data area
	Dynamically declaring an array to get a ptr allocates it on the heap
2d array
	get weord of dynamic alloc
IN C str are arrays
	A str is a ptr to an array of chars
	Strings can eb indexed and passed by reference
You can do pointers to pointers
	char **argv is a pointer to a pointer of characters
Multidimensional array
	row major order
	IT will be contiguous
	3d array
		plane->row-> column
		These grow very very quickly
Function pointers
	Points to executable code in mem
	Derefing a function ptr yields the referenced fuction.
	Notice how there are paren around the function pointer
		Without them the function pointer would be parsed as func declera
	Function ptr things 
		Can have a pointer to a function
Summary
	ptr contains a meme address
	no addy ptr is null
	the addy of a var is with &
	the obj of a ptr ptrs to can be derefed by the ptr
	Arby levels of pting pointer can point to pointer etc
	pass big data structs

Computational Complexity
	Size of the problem
	We talk abt size of input
	Some common functions
	log(x) x x log(x) x^2 etc
		speed of grow
	factorial time is yucky
Binary search
	log(n) time
Find minimum order n
Sorts
	nlog(n)
	n^2 for bubble
	n^3 for matrix mult
	Enumerate permutations n!
Formale math
	f(n) is in the set O if O(g(n))
 	O = {f :  there exists n0 and c such that 0<=f(n) <= c*g(n) for all n>n0}
Simple linear algorithm
	search in an array for one element
	Why is order n? Loop, loops through
Quadratic alg
	O(n^2) since loop inside loop
A less simple algorithm
	On avg nlong(n)
	O(n^2)
	Quicksort
		If only take off one each time, then its n^2 since loop for every
		Best case, cut in half
			Then logn
Estimate by looking at loops and recursion
	We multiply the complexity of loops nested in loops
	One lopp usualy adds O(n)
	two loops are O(n^2)
	three are O(n^3)
	etc.
Addition doesn't matter
	So what about c
	C is an overhead alg needs
	But it is only constanst speed and a little better.
	Do a better algorithm
Consider x^2 - 10xlog(X)
	Then sometimes x^2 is faster
	Use buble sort of less than 50 item
	depends on alg
	Best case for bubble osrt is order n, but worst is n^2 dont do it for biggie things
So what abt recursion	
	order n?
Fibonacci function
	O(2^n)
		why?
		Each thing splits into tree, so approx is exponential to compute
	fib n remember 
		can be O(n)
	time space stuff time taken vs space
	WE can calculate analytically
Sqrt
	How to calculate?
	Number between 0 and 1
	while of cut in half loop + how mayn times divide by 2 for epsilon
	For big number bad maybe?
IF we go back 3000 years, Euclid has a mthod that each time it divides and gets faster
We know that integer factorization is hard
	Sometimes we don't know the best algorithms
	if someone can factorise hella we have an issue
	become conversant with computatonal complexity in space and time
	time matters

